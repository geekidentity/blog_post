---
categories: Deep Learning

tags: 
  - AI
  - 深度学习工程师
  - 学习笔记

title: 吴恩达(Andrew Ng)深度学习工程师课程目录

date: 2017-12-11
---

# 第 1 门课程：神经网络和深度学习（Neural Networks and Deep Learning）

如果你没有人工智能（AI）的专业背景却又想要涉足AI行业，那么这门微专业正适合你。现如今，深度学习工程师已经成为了非常热门的职业，掌握深度学习将会为你带来大量的工作机会。甚至可以说，深度学习是一种新兴的“超能力”，它可以能让你搭建出自己的AI系统，而这在几年前还是令人难以想象的事情。

## 课程概述

这是深度学习工程师微专业中的第一门课。

这门课将为你介绍深度学习的基础知识。学完这门课，你将能够：
* 理解驱动深度学习的主要技术趋势。
* 能够搭建、训练并且运用全连接的深层神经网络。
* 了解如何实现高效的（向量化）的神经网络。
* 理解神经网络架构中的关键参数。
 
这门课将会详尽地介绍深度学习的基本原理，而不仅仅只进行理论概述。

当你完成这门微专业之后，你就能够将深度学习运用到你的个人应用中，调教出属于你自己的AI。

如果你正在找与人工智能有关的工作，那么，在学习完这门课后，应对面试官提问的基础面试问题就绰绰有余了。

## 预备知识
需要有基本的数学、编程和机器学习基础。

## 第一周  深度学习概论（Introduction to deep learning）

学习驱动神经网络兴起的主要技术趋势，了解现今深度学习在哪里应用、如何应用。

### 1.1  [欢迎来到深度学习工程师微专业（Welcome）](http://blog.geekidentity.com/deeplearning_specialization/1_neural-networks-deep-learning/week1/1_welcome/)
### 1.2  什么是神经网络？（What is a neural network?）
### 1.3  用神经网络进行监督学习（Supervised Learning with Neural Networks）
### 1.4  为什么深度学习会兴起？（Why is Deep Learning taking off?）
### 1.5  关于这门课（About this Course）
### 1.6  课程资源（Course Resources）

 
## 第二周  神经网络基础（Neural Networks Basics）

学习如何用神经网络的思维模式提出机器学习问题、如何使用向量化加速你的模型。

### 2.1  二分分类（Binary Classification）
### 2.2  logistic 回归（Logistic Regression）
### 2.3  logistic 回归损失函数（Logistic Regression Cost Function）
### 2.4  梯度下降法（Gradient Descent）
### 2.5  导数（Derivatives）
### 2.6  更多导数的例子（More Derivative Examples）
### 2.7  计算图（Computation graph）
### 2.8  计算图的导数计算（Derivatives with a Computation Graph）
### 2.9  logistic 回归中的梯度下降法（Logistic Regression Gradient Descent）
### 2.10  m 个样本的梯度下降（Gradient Descent on m Examples）
### 2.11  向量化（Vectorization）
### 2.12  向量化的更多例子（More Vectorization Examples）
### 2.13  向量化 logistic 回归（Vectorizing Logistic Regression）
### 2.14  向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression's Gradient Output）
### 2.15  Python 中的广播（Broadcasting in Python）
### 2.16  关于 python / numpy 向量的说明（A note on python/numpy vectors）
### 2.17  Jupyter / Ipython 笔记本的快速指南（Quick tour of Jupyter/iPython Notebooks）
### 2.18  （选修）logistic 损失函数的解释（Explanation of logistic regression cost function (optional)）

 
## 第三周  浅层神经网络（Shallow neural networks）

学习使用前向传播和反向传播搭建出有一个隐藏层的神经网络。

### 3.1  神经网络概览（Neural Networks Overview）
### 3.2  神经网络表示（Neural Network Representation）
### 3.3  计算神经网络的输出（Computing a Neural Network's Output）
### 3.4  多样本向量化（Vectorizing across multiple examples）
### 3.5  向量化实现的解释（Explanation for Vectorized Implementation）
### 3.6  激活函数（Activation functions）
### 3.7  为什么需要非线性激活函数？（Why do you need non-linear activation functions?）
### 3.8  激活函数的导数（Derivatives of activation functions）
### 3.9  神经网络的梯度下降法（Gradient descent for Neural Networks）
### 3.10  （选修）直观理解反向传播（Backpropagation intuition (optional)）
### 3.11  随机初始化（Random Initialization）

 
## 第四周  深层神经网络（Deep Neural Networks）

理解深度学习中的关键计算，使用它们搭建并训练深层神经网络，并应用在计算机视觉中。

### 4.1  深层神经网络（Deep L-layer neural network）
### 4.2  深层网络中的前向传播（Forward Propagation in a Deep Network）
### 4.3  核对矩阵的维数（Getting your matrix dimensions right）
### 4.4  为什么使用深层表示（Why deep representations?）
### 4.5  搭建深层神经网络块（Building blocks of deep neural networks）
### 4.6  前向和反向传播（Forward and Backward Propagation）
### 4.7  参数 VS 超参数（Parameters vs Hyperparameters）
### 4.8  这和大脑有什么关系？（What does this have to do with the brain?）


## 大师访谈

我在本微专业中采访了多位人工智能领域大师，希望爱好人工智能的你可以从他们的睿智回答中了解 AI 领域的专业建议。


### Geoffrey Hinton

![Geoffrey Hinton](http://edu-image.nosdn.127.net/91B25B8114F3CF3E0EBB27B7152FD1E8.png?imageView&thumbnail=520x520&quality=100)


计算机学家、心理学家，盖茨比计算神经科学中心的创始人，多伦多大学计算机科学系教授，2013年加入谷歌，以神经网络方面的杰出贡献闻名，是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。他在edX上的课程很受欢迎。

### Pieter Abbeel

![Pieter Abbeel](http://edu-image.nosdn.127.net/BC51EB4B90CC9E6E0A747AF955E9424D.png?imageView&thumbnail=520x520&quality=100)

加州大学伯克利分校计算机系副教授。斯坦福大学计算机学系博士，师从 Andrew Ng。主要关注机器人学习，在edX上有机器学习初级课程。

### Ian Goodfellow

![Ian Goodfellow](http://edu-image.nosdn.127.net/B85D537D7E31174EF5DFE20293679A1D.jpg?imageView&thumbnail=520x520&quality=100)

Google Brain 研究员，《深度学习》教科书的第一作者，生成对抗网络（GAN）提出者。

# 第 2 门课程：改善深层神经网络：超参数调试、正则化以及优化（Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization）

这门课会带领你从理解深度学习的理论到成功实践。深度学习过程并不是一个黑箱，你会理解什么在驱动网络，并且能够更系统化地得到好的运行结果。同时，你也会学习如何使用TensorFlow。

## 课程概述

这是深度学习工程师微专业中的第二门课。

学完这门课之后，你将会:

* 理解业界构建深度神经网络应用最有效的做法。
* 能够高效地使用神经网络通用的技巧，包括初始化、L2和dropout正则化、Batch归一化、梯度检验。
* 能够实现并应用各种优化算法，例如mini-batch、Momentum、RMSprop和Adam，并检查它们的收敛程度。
* 理解深度学习时代关于如何构建训练/开发/测试集以及偏差/方差分析最新最有效的方法。
* 能够用TensorFlow实现一个神经网络。

这门课将会详尽地介绍深度学习的基本原理，而不仅仅只进行理论概述。

当你完成这门微专业之后，你就能够将深度学习运用到你的个人应用中，调教出属于你自己的AI。

如果你正在找与人工智能有关的工作，那么，在学习完这门课后，应对面试官提问的基础面试问题就绰绰有余了。

## 预备知识

需要有基本的数学、编程和机器学习基础。

## 第一周  深度学习的实用层面（Practical aspects of Deep Learning）

### 1.1  训练/开发/测试集（Train / Dev / Test sets）
### 1.2  偏差/方差（Bias / Variance）
### 1.3  机器学习基础（Basic Recipe for Machine Learning）
### 1.4  正则化（Regularization）
### 1.5  为什么正则化可以减少过拟合？（Why regularization reduces overfitting?）
### 1.6  Dropout 正则化（Dropout Regularization）
### 1.7  理解 Dropout（Understanding Dropout）
### 1.8  其他正则化方法（Other regularization methods）
### 1.9  正则化输入（Other regularization methods）
### 1.10  梯度消失与梯度爆炸（Vanishing / Exploding gradients）
### 1.11  神经网络的权重初始化（Vanishing / Exploding gradients）
### 1.12  梯度的数值逼近（Numerical approximation of gradients）
### 1.13  梯度检验（Gradient checking）
### 1.14  关于梯度检验实现的注记（Gradient checking）


## 第二周  优化算法（Optimization algorithms）

### 2.1  Mini-batch 梯度下降法（Mini-batch gradient descent）
### 2.2  理解 mini-batch 梯度下降法（Understanding mini-batch gradient descent）
### 2.3  指数加权平均（Exponentially weighted averages）
### 2.4  理解指数加权平均（Exponentially weighted averages）
### 2.5  指数加权平均的偏差修正（Bias correction in exponentially weighted averages）
### 2.6  动量梯度下降法（Gradient descent with momentum）
### 2.7  RMSprop
### 2.8  Adam 优化算法（Adam optimization algorithm）
### 2.9  学习率衰减（Adam optimization algorithm）
### 2.10  局部最优的问题（The problem of local optima）


## 第三周  超参数调试、Batch正则化和程序框架（Hyperparameter tuning, Batch Normalization and Programming Frameworks）

### 3.1  调试处理（Tuning process）
### 3.2  为超参数选择合适的范围（Using an appropriate scale to pick hyperparameters）
### 3.3  超参数训练的实践：Pandas VS Caviar（Using an appropriate scale to pick hyperparameters）
### 3.4  正则化网络的激活函数（Normalizing activations in a network）
### 3.5  将 Batch Norm 拟合进神经网络（Fitting Batch Norm into a neural network）
### 3.6  Batch Norm 为什么奏效？（Why does Batch Norm work?）
### 3.7  测试时的 Batch Norm（Batch Norm at test time）
### 3.8  Softmax 回归（Softmax Regression）
### 3.9  训练一个 Softmax 分类器（Training a softmax classifier）
### 3.10  深度学习框架（Deep learning frameworks）
### 3.11  TensorFlow


## 大师访谈
 
我在本微专业中采访了多位人工智能领域大师，希望你可以从他们的睿智回答中了解 AI 领域的专业建议。

### Yoshua Bengio

![Yoshua Bengio](http://edu-image.nosdn.127.net/A660E019055DE9C744588758413FCA52.jpg?imageView&thumbnail=520x520&quality=100)

加拿大蒙特利尔大学教授，蒙特利尔大学机器学习研究所（MILA）的负责人，神经网络三巨头之一，人工智能孵化器 Element AI 联合创始人之一，2017年成为微软人工智能研究顾问。


### 林元庆

![林元庆](http://edu-image.nosdn.127.net/97F3359340660F227CF684C735CE3D2C.jpg?imageView&thumbnail=520x520&quality=100)

现任百度深度学习实验室（IDL）主任，曾任 NEC 美国实验室媒体分析部门主管。在他的带领下 NEC 研究团队在深度学习、计算机视觉和无人驾驶等领域取得世界领先水平。


# 第 ３ 门课程：结构化机器学习项目（Structuring Machine Learning Projects）

你将在本课程中学习如何建立一个机器学习项目。如果你想成为一个AI领域的技术大牛，以及知道如何带领你的团队开展工作，那么千万不要错过这门课。

## 课程概述
这是深度学习工程师微专业的第三门课。

本课程的很多内容都是首次作为教学内容，并且很多都源自我个人在构建和运营深度学习产品方面的经验。这门课也有两个“流程模拟器”，可以让你作为机器学习项目的领导者，练习如何决策。这会让你收获极其宝贵的“行业经验”，一般来说，这些经验可能要通过多年的工作积累才能得到。

在2周的学习之后，你会：

* 理解如何诊断机器学习系统中的错误
* 能够优先减小误差最有效的方向
* 理解复杂ML设定，例如训练/测试集不匹配，比较并/或超过人的表现
* 知道如何应用端到端学习、迁移学习以及多任务学习

很多团队浪费数月甚至数年来理解这门课所教授的准则，也就是说，这门两周的课可以为你节约数月的时间。

## 预备知识

需要有基本的数学、编程和机器学习基础。

## 第一周  机器学习（ML）策略（1）（ML Strategy (1)）

### 1.1  为什么是ML策略（Why ML Strategy）
### 1.2  正交化（Orthogonalization）
### 1.3  单一数字评估指标（Single number evaluation metric）
### 1.4  满足和优化指标（Satisficing and Optimizing metric）
### 1.5  训练/开发/测试集划分（Satisficing and Optimizing metric）
### 1.6  开发集合测试集的大小（Size of the dev and test sets）
### 1.7  什么时候该改变开发/测试集和指标（When to change dev/test sets and metrics）
### 1.8  为什么是人的表现（Why human-level performance?）
### 1.9  可避免偏差（Avoidable bias）
### 1.10  理解人的表现（Understanding human-level performance）
### 1.11  超过人的表现（Surpassing human-level performance）
### 1.12  改善你的模型的表现（Improving your model performance）


## 第二周  机器学习（ML）策略（2）（ML Strategy (2)）

### 2.1  进行误差分析（Carrying out error analysis）
### 2.2  清楚标注错误的数据（Cleaning up incorrectly labeled data）
### 2.3  快速搭建你的第一个系统，并进行迭代（Build your first system quickly, then iterate）
### 2.4  在不同的划分上进行训练并测试（Training and testing on different distributions）
### 2.5  不匹配数据划分的偏差和方差（Bias and Variance with mismatched data distributions）
### 2.6  定位数据不匹配（Addressing data mismatch）
### 2.7  迁移学习（Transfer learning）
### 2.8  多任务学习（Multi-task learning）
### 2.9  什么是端到端的深度学习（Multi-task learning）
### 2.10 是否要使用端到端的深度学习（Multi-task learning）


## 大师访谈


### Andrej Karpathy

![Andrej Karpathy](http://edu-image.nosdn.127.net/D4C5647E56215826670C4FB7888F0763.jpg?imageView&thumbnail=520x520&quality=100)

斯坦福大学计算机系博士，师从李飞飞，自2017年任特斯拉人工智能研究部门总监。


### Ruslan Salakhutdinov

![Ruslan Salakhutdinov](http://edu-image.nosdn.127.net/1C7366FE763ABE98443D0CCDF095A5DA.jpg?imageView&thumbnail=520x520&quality=100)

卡内基梅隆大学副教授，苹果人工智能研究部门总监。

# 第 ４ 门课程：卷积神经网络（Convolutional Neural Networks）

这门课会教你如何搭建卷积神经网络并将其应用到图像数据上。这两年，在深度学习的基础上，计算机视觉也得到了迅猛的发展，从而产生了大量令人振奋的应用：安全无人驾驶、精确面部识别、自动阅读放射成像等等。

## 课程概述
这是深度学习工程师微专业的第四门课。

通过这门课的学习，你将会：
- 理解如何搭建一个神经网络，包括最新的变体，例如残余网络
- 知道如何将卷积网络应用到视觉检测和识别任务。
- 知道如何使用神经风格迁移生成艺术。
- 能够在图像、视频以及其他2D或3D数据上应用这些算法。

## 预备知识

需要有基本的数学、编程和机器学习基础。

## 第一周  卷积神经网络（Foundations of Convolutional Neural Networks）

### 1.1  计算机视觉（Computer Vision）
### 1.2  边缘检测示例（Computer Vision）
### 1.3  更多边缘检测内容（More Edge Detection）
### 1.4  Padding
### 1.5  卷积步长（Strided Convolutions）
### 1.6  卷积的“卷”体现之处（Convolutions Over Volume）
### 1.7  单层卷积网络（One Layer of a Convolutional Network）
### 1.8  简单卷积网络示例（Simple Convolutional Network Example）
### 1.9  池化层（Pooling Layers）
### 1.10  卷积神经网络示例（CNN Example）
### 1.11  为什么使用卷积？（Why Convolutions?）

## 第二周  深度卷积网络：实例探究（Deep convolutional models: case studies）

### 2.1  为什么要进行实例探究（Why look at case studies?）
### 2.2  经典网络（Classic Networks）
### 2.3  残差网络（ResNets）
### 2.4  残差网络为什么有用？（Why ResNets Work）
### 2.5  网络中的网络以及 1×1 卷积（Networks in Networks and 1x1 Convolutions）
### 2.6  谷歌 Inception 网络简介（Inception Network Motivation）
### 2.7  Inception 网络（Inception Network）
### 2.8  使用开源的实现方案（Using Open-Source Implementation）
### 2.9  迁移学习（Transfer Learning）
### 2.10  数据扩充（Data Augmentation）
### 2.11  计算机视觉现状（State of Computer Vision）

## 第三周  目标检测（Object detection）

### 3.1  目标定位（Object Localization）
### 3.2  特征点检测（Landmark Detection）
### 3.3  目标检测（Object Detection）
### 3.4  卷积的滑动窗口实现（Convolutional Implementation of Sliding Windows）
### 3.5  Bounding Box预测（Bounding Box Predictions）
### 3.6  交并比（Intersection Over Union）
### 3.7  非极大值抑制（Non-max Suppression）
### 3.8  Anchor Boxes
### 3.9  YOLO 算法（YOLO Algorithm）
### 3.10  RPN网络（(Optional) Region Proposals）

## 第四周  特殊应用：人脸识别和神经风格转换（Special applications: Face recognition & Neural style transfer）

### 4.1  什么是人脸识别？（What is face recognition?）
### 4.2  One-Shot 学习（One Shot Learning）
### 4.3  Siamese 网络（Siamese Network）
### 4.4  Triplet 损失（Triplet Loss）
### 4.5  面部验证与二分类（Face Verification and Binary Classification）
### 4.6  什么是神经风格转换？（What is neural style transfer?）
### 4.7  什么是深度卷积网络？（What is neural style transfer?）
### 4.8  代价函数（Cost Function）
### 4.9  内容代价函数（Content Cost Function）
### 4.10  风格代价函数（Style Cost Function）
### 4.11 一维到三维推广（1D and 3D Generalizations）


# 第 5 门课程：序列模型（Sequence Models）

这门课会教你如何构建自然语言、音频以及其他序列数据的模型。基于深度学习，序列算法比两年前有了巨大的飞跃，并且催生了语音识别、音乐合成、聊天机器人、机器翻译、自然语言理解等领域的诸多应用。

## 课程概述
这是深度学习工程师微专业的第五门课。

通过这门课的学习，你将会：

- 理解如何构建并训练循环神经网络（RNN），以及一些广泛应用的变体，例如GRU和LSTM
- 能够将序列模型应用到自然语言问题中，包括文字合成。
- 能够将序列模型应用到音频应用，包括语音识别和音乐合成。

## 预备知识
需要有基本的数学、编程和机器学习基础。
