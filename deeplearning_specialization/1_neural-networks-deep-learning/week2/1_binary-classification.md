---
categories: 深度学习工程师

tags: 
  - AI
  - 深度学习工程师
  - 学习笔记
  - 二分分类

title: 神经网络和深度学习-第二周神经网络基础-第一节：二分分类

date: 2017-12-18
---

本系列博客是吴恩达(Andrew Ng)[深度学习工程师](http://mooc.study.163.com/smartSpec/detail/1001319001.htm) 课程笔记。全部课程请查看[吴恩达(Andrew Ng)深度学习工程师课程目录](http://blog.geekidentity.com/deeplearning_specialization/catalogues/)

本周，我们会学习神经网络编程的基础知识。当你要构建一个神经网络，有些技巧是相当重要的。例如，如果你有m个样本的训练集，你可能会习惯性地去用一个for循环来遍历这m个样本。但事实上实现一个神经网络，如果你要遍历整个训练集，燕不需要直接使用for循环。在本周的课程，你会学到如何做到。

另外，在神经网络的计算过程中，通常有一个正向过程或者叫正向传播步骤，接着会有一个反向步骤也叫做反向传播步骤。在本周的课程中会讲解为什么神经网络的计算过程可以分为正向传播和反向传播两个分开的过程。

在本周课程中，我会用logistic回归来阐述，以便你能更好地理解。如果你之前学习过logistic回归，我也认为，这周的学习材料也会带给你一些新的、有意思的想法。下面正式开始。

Logistic回归是一个用于二分分类的算法，我们从一个问题开始。这里有一个二分分类问题的例子，例如你有一张图片作为输入，就像这样，你想输出识别此图的标签，如果是猫输出1，如果不是则输出0，我们用$y$来表示输出的结果标签：

![识别猫](http://blog.geekidentity.com/images/deeplearning_specialization/neural-networks-deep-learning/week2/1_binary-classification/recognize_cat.png)

我们看看一张照片在计算机中是如何表示的。计算机保存一张图片，要保存三个独立矩阵分别对应图中红蓝绿三个颜色通道。如果图片是64\*64像素的，就有三个64\*64矩阵对应图片中的红绿蓝三种像素的亮度。为了方便展示，这里的示例用三个小矩阵：

![三原色矩阵](http://blog.geekidentity.com/images/deeplearning_specialization/neural-networks-deep-learning/week2/1_binary-classification/three_matrixs.png)

要把这些像素的亮度值放入一个特征向量中，就要把这些像素值都提取出来，放入一个特征向量x，为了把这些像素值取出放入特征向量中需要定义一个特征向量x以表示这张图片

把图片中所有像素强度值都列出来，如果图片是$64 \times 64$的，那么向量$x$的总维度就是$64 \times64 \times3=12288$，因为这是三个矩阵的元素数量。我们用$n_x=12288$来表示输入特征向量x的维度，有时候为了简洁，我会直接用小写的n表示输入特征向量的维度。

在二分类问题中目标是训练出一个分类器，它以图片的特征向量`x`作为输入，预测输出的结果是标签y（0或1），也就是说预测图片中是否有猫。

现在我们看看在后面课程中需要用到的一些符号。用一对(x, y)表示一个单独的样本，其中$x$是$n_x$维的向量，标签y值为0或1。

$$
(x,y) \qquad x \in R^{n_x},y \in \lbrace 0, 1 \rbrace
$$

训练集由m个训练样本构成，$(x^{(1)}, y^{(1)})$表示第一个样本，$(x^{(2)}, y^{(2)})$表示第二个样本...$(x^{(m)}, y^{(m)})$表示最后一个样本$m$，这些一起就表示整个训练集。

$$
m \; training \; example: \; (x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}), \, ... \, ,(x^{(m)}, y^{(m)})
$$

用小写字母$m$ 表示训练集的个数，有时为了强调这个字母是训练集的个数可以写作$m_{train}$，用$m_{test}$表示测试集的样本数量。最后，用更紧凑的符号表示训练集，我们定义一个矩阵，用大写的$X$表示，它由训练集中的$x^{(1)},x^{(2)}, \, ... \, x^{(m)}$组成：

$$
X= \begin{bmatrix}
    \mid & \mid & \quad & \mid \\
    x^{(1)} & x^{(2)} & ...  & x^{(m)} \\
    \mid & \mid & \quad  & \mid \\
\end{bmatrix}
\qquad X \in R^{n_x \times m}
$$

这个矩阵由m列组成，高度为$n_x$，

> 注意：有时候矩阵$X$的定义是训练样本作为行向量堆叠，而不是这样列向量堆叠。在构建神经网络时，用上述这种约定形式，会让构建过程列简单。

当你用Python实现的时候你会看到`X.shape()`，这条Python命令用来输出矩阵的维度，即$(n_x,m)$。

这就是如何将训练样本，即输入$x$用矩阵表示，对于输出标签$y$同样为了方便构一个建神经网络将$y$标签也放到列中，所以我们定义

$$
Y=[y^{(1)},y^{(2)}, \, ... \, y^{(m)}] \qquad  Y \in R^{1 \times m}
$$

在后面的课程中，要实现神经网络，你会发现好的惯例符号能够将不同训练样本的数据联系起来，这里说的数据不仅有$x,y$，还会有之后其他变量，将不同的训练样本数据取出来，放到不同的列上，就像刚刚我们处理$x,y$那样。

这门课程中在logistic回归和神经网络要用到的符号就这些了如果你忘记了这些符号的意义，我们也会在课程网站上放上符号说明，方便快速查阅每个符号的意义。下个课程将以logistic回归作为开始。
