---
categories: 深度学习工程师

tags: 
  - AI
  - 深度学习工程师
  - 学习笔记
  - 计算图
  - 导数计算


title: 神经网络和深度学习-第二周神经网络基础-第八节：计算图的导数计算

date: 2018-01-01
---

本系列博客是吴恩达(Andrew Ng)[深度学习工程师](http://mooc.study.163.com/smartSpec/detail/1001319001.htm) 课程笔记。全部课程请查看[吴恩达(Andrew Ng)深度学习工程师课程目录](http://blog.geekidentity.com/deeplearning_specialization/catalogues/)

 上一节中，我们使用计算图来计算函数$J$，现在我们理清一下计算图的描述，看看我们如何利用它计算出函数$J$的导数。

下图是一个流程图，假设你要计算$J$对$v$的导数$\frac{dJ}{dv}$，比如我们改变$v$值那么$J$的值怎么呢？定义上$J$是$3v$，现在$v=11$，所以如果让$v$增加一点点，比如到11.001，那么$J$增加到33.003。所以这里$v$增加了1.001，最终结果是$J$上升到原来3倍，所以$J$对$v$的导数等于3。因为对于任何$v$的增量，$J$都会有三倍增量。

![](http://blog.geekidentity.com/images/deeplearning_specialization/neural-networks-deep-learning/week2/8_derivatives-with-a-computation-graph/computing-derivatives.png)

在反向传播算法中，我们看到如果你想计算最后输出变量的导数，使用你最关心的变量对v的导数。那么我们就做完了一步反向传播，所以在这个流程图中是一个反向步。

我们来看另一个例子，$\frac{dJ}{da}$又是多少？换句话说，如果我们提高a的数值，对$J$的数值有什么影响？我们看看这个例子变量$a=5$，我们让它增加到5.001，那么对v的影响就是$a+U=11.001$，$J$就变成了33.003了。所以我们看到的是如果你让a增加0.001，$J$增加0.003。所以可以看出，当a变化时，其变化会传播到流程图的最右边。所以$J$的增量是3乘以a的增量，这意味着导数是3。要解释这样计算过程其中一个方式就是如果你改变了a，那也会改变v，通过改变v也会改变$J$。这在微积分里实际上叫链式法则，就是当你改变a时v的变化量乘以改v时J的变化量。我们从这个计算中看到如果你让a增加0.001，v也会变化相同的大小所以$\frac{dv}{da}=1$。

现在我们介绍一种新的符号约定，当你编程实现反向传播时，通常会有一个最终输出值是你要关心的或要优化的（Final output variable），在这种情况下，最终的输出变量是J（计算图里最后一个符号）。所以，有很多计算方试计算输出变量的导数。所以d FinalOutputVar是对某个变量的导数，我们就用d var表示。所以在很多计算中你需要计算最终输出结果的导数，在这个例子里是$J$，还有各种中间变量，比如a b c u v。当你在软件里实现的时候，变量名叫什么？你可以做的一件事是，在Python中，可以用dJ/dvar，但因为你一直对dJ求导，对这个最终输出变量求导，这里要介绍一个新的符号，在程序里我们就使用变量名dvar来表示你关心的最终变量的导数（var可能是J/L等）。

所以在代码中dv=3，dJ=3。通过这个流程图部分完成的后向传播算法，我们在下一节看这个例子剩下的部分。

